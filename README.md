# Robotics-RL-FMs-Integration
This repository contains a curated list of the papers classified in the survey titled ***"Integrating Reinforcement Learning with Foundation Models for Autonomous Robotics: Methods and Perspectives"***. We also provide five Excel files (one for each category) that offer detailed summaries of the analyses we performed using the paper's taxonomy. These summaries cover several features of the analyzed papers, such as `name of the framework`, `model used`, `code availability`, `dataset`, `type of application`, `simulation vs. real-world`, `subcategories`, `experiment evaluation`, `year of publication`, `RL for FM vs. FM for RL`, and `short description`.
## Abstract
Large pre-trained models, such as foundation models (FMs), despite their powerful abilities to understand complex patterns and generate sophisticated outputs, often struggle with adapting to specific tasks. Reinforcement learning (RL), which allows agents to learn through interaction and feedback, presents a compelling solution. Integrating RL empowers foundation models to achieve desired outcomes and excel at specific tasks. Simultaneously, RL itself can be enhanced when coupled with the reasoning and generalization capabilities of FMs. The synergy between foundation models and RL is revolutionizing many fields, robotics is among them. Foundation models, rich in knowledge and generalization capabilities, provide robots with a wealth of information, while RL enables them to learn and adapt through real-world interaction. This survey paper offers a comprehensive exploration of this exciting intersection, examining how these paradigms can be integrated to push the boundaries of robotic intelligence. We analyze the use of foundation models as action planners, the development of robotics-specific foundation models, and the mutual benefits of combining foundation models with RL. We also present a taxonomy of integration approaches, including large language models, vision-language models, diffusion models, and transformer-based RL models. Finally, we delve into how RL can harness the world representations learned from foundation models to enhance robotic task execution. Through synthesizing current research and highlighting key challenges, this survey aims to spark future research and contribute to the development of more intelligent, adaptable, and capable robotic systems. To summarize the analysis conducted in this work, we also provide a continuously updated collection of papers based on our taxonomy.
## 1. Large Language Models Enhance Reasoning Capabilities in RL Agents
 ### 1.1 Inverse RL: generating the reward function through LLMs
 ### 1.2 Large language models to directly generate or refine RL policies
 ### 1.3 Grounding LLM plans in real world through RL generated primitives
## 2. Vision Language Models for RL-Based Decision Making
## 3. RL Robot Control Empowered by Diffusion Models
 ### 3.1 Diffusion models for policy generation
 ### 3.2 Diffusion models for policy representation
 ### 3.3 Diffusion models for planning
 ### 3.4 Diffusion models for offline RL
 ### 3.5 Diffusion models for value function estimation, reward function generation and others
## 4. Reinforcement Learning Leverages Video Prediction and World Models
 ### 4.1 Learning robotic tasks with video prediction
 ### 4.2 Foundation world models for model-based RL
## 5. Transformer Reinforcement Learning Models

## Citation

If you find our project useful, please cite our paper:

```

```
